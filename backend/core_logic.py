import os
import time
import json
import re
from dotenv import load_dotenv
from typing import List, Dict, Union

# --- Imports ---
# D√πng pdfplumber tr·ª±c ti·∫øp ƒë·ªÉ ki·ªÉm so√°t t·ªët h∆°n vi·ªác ƒë·ªçc file
import pdfplumber 
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
# Import Safety Settings ƒë·ªÉ tr√°nh Gemini ch·∫∑n CV
from langchain_google_genai import HarmBlockThreshold, HarmCategory
from langchain_chroma import Chroma
# V·∫™N D√ôNG HUGGING FACE CHO EMBEDDINGS (Offline CPU)
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from pydantic import BaseModel, Field

load_dotenv()

# --- DATA MODELS ---
class JobMatchResult(BaseModel):
    personal_info: Dict[str, str] = Field(description="Name, position, experience extracted from CV")
    matching_score: Dict[str, Union[int, str]] = Field(description="Percentage score and explanation")
    requirements_breakdown: Dict[str, str] = Field(description="Ratios for must-have and nice-to-have criteria")
    matched_keywords: List[str] = Field(description="List of matching technical skills")
    radar_chart: Dict[str, int] = Field(description="Scores 1-10 for 5 dimensions")
    radar_reasoning: Dict[str, str] = Field(description="Explanation for each radar score in Vietnamese")
    bilingual_content: Dict[str, Union[Dict, List]] = Field(description="Assessment content in EN and VI")
    
CORE_PROMPT = """
B·∫°n l√† m·ªôt Tr·ª£ l√Ω Tuy·ªÉn d·ª•ng AI chuy√™n nghi·ªáp (JobMatchr). Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch CV (ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi d·∫°ng text) v√† M√¥ t·∫£ c√¥ng vi·ªác (JD - m·ªói d√≤ng l√† m·ªôt y√™u c·∫ßu).

**INPUT DATA:**
1. CV Text: {cv_text}
2. JD Text: {jd_text} (L∆∞u √Ω: M·ªói d√≤ng trong JD l√† m·ªôt ti√™u ch√≠ ri√™ng bi·ªát).

**NHI·ªÜM V·ª§:**
H√£y th·ª±c hi·ªán c√°c b∆∞·ªõc sau m·ªôt c√°ch logic:

B∆Ø·ªöC 1: TR√çCH XU·∫§T TH√îNG TIN C√Å NH√ÇN
- T√¨m Name, Position (V·ªã tr√≠ ·ª©ng tuy·ªÉn/hi·ªán t·∫°i), Experience (T·ªïng s·ªë nƒÉm kinh nghi·ªám - ch·ªâ l·∫•y s·ªë).

B∆Ø·ªöC 2: PH√ÇN T√çCH JD V√Ä T√çNH ƒêI·ªÇM (QUY T·∫ÆC "1 ƒê·ªÄU")
- T√°ch JD th√†nh c√°c d√≤ng ri√™ng bi·ªát. T·ªïng s·ªë d√≤ng = T·ªïng y√™u c·∫ßu (Total_Req).
- Ph√¢n lo·∫°i t·ª´ng d√≤ng th√†nh "B·∫Øt bu·ªôc" (Requirement) ho·∫∑c "∆Øu ti√™n" (Nice-to-have) d·ª±a tr√™n t·ª´ kh√≥a:
  * Ti·∫øng Anh: "nice to have", "plus", "preferred", "advantage", "desired", "bonus", "optional", "willing to".
  * Ti·∫øng Vi·ªát: "∆∞u ti√™n", "l·ª£i th·∫ø", "ƒëi·ªÉm c·ªông", "kh√¥ng b·∫Øt bu·ªôc", "mong mu·ªën", "n·∫øu c√≥".
  -> C√°c tr∆∞·ªùng h·ª£p c√≤n l·∫°i m·∫∑c ƒë·ªãnh l√† **"B·∫Øt bu·ªôc" (Requirement)**.
- ƒê·ªëi chi·∫øu CV: V·ªõi m·ªói d√≤ng JD, n·∫øu CV c√≥ b·∫±ng ch·ª©ng ƒë√°p ·ª©ng => T√≠nh l√† 1 ƒëi·ªÉm (Matched).
- Keyword ph√°t hi·ªán: Tr√≠ch xu·∫•t c√°c t·ª´ kh√≥a k·ªπ thu·∫≠t (Hard skill) tr√πng kh·ªõp gi·ªØa CV v√† JD.
- C√¥ng th·ª©c t√≠nh % chung: (T·ªïng s·ªë d√≤ng Matched / T·ªïng s·ªë d√≤ng JD) * 100.

B∆Ø·ªöC 3: ƒê√ÅNH GI√Å SONG NG·ªÆ (ANH & VI·ªÜT)
- T·∫°o n·ªôi dung ƒë√°nh gi√° cho c√°c m·ª•c: ƒê√°nh gi√° chung, ƒêi·ªÉm m·∫°nh, ƒêi·ªÉm y·∫øu (Missing skills), C√¢u h·ªèi ph·ªèng v·∫•n.
- N·ªôi dung Ti·∫øng Anh vi·∫øt tr∆∞·ªõc, Ti·∫øng Vi·ªát d·ªãch s√°t nghƒ©a theo sau.

B∆Ø·ªöC 4: CH·∫§M ƒêI·ªÇM RADAR (1-10) V√Ä GI·∫¢I TH√çCH L√ù DO
*B·∫Øt bu·ªôc ch·∫•m d·ª±a tr√™n Barem sau:*
1. **Hard Skills (K·ªπ nƒÉng c·ª©ng):** 1-4 (Thi·∫øu nhi·ªÅu), 5-7 (C∆° b·∫£n), 8-10 (ƒê·∫ßy ƒë·ªß/N√¢ng cao).
2. **Soft Skills (K·ªπ nƒÉng m·ªÅm):** 1-4 (S∆° s√†i), 5-7 (C√≥ nh·∫Øc ƒë·∫øn), 8-10 (C√≥ v√≠ d·ª• c·ª• th·ªÉ).
3. **Experience (Kinh nghi·ªám):** 1-4 (√çt/Tr√°i ng√†nh), 5-7 (T∆∞∆°ng ƒë·ªëi), 8-10 (V∆∞·ª£t y√™u c·∫ßu).
4. **Education (H·ªçc v·∫•n):** 1-4 (Kh√¥ng li√™n quan), 5-7 (ƒê√∫ng ng√†nh), 8-10 (B·∫±ng c·∫•p cao/Ch·ª©ng ch·ªâ x·ªãn).
5. **Domain Knowledge (Hi·ªÉu bi·∫øt ng√†nh):** 1-4 (Chung chung), 5-7 (Hi·ªÉu quy tr√¨nh), 8-10 (Am hi·ªÉu nghi·ªáp v·ª• s√¢u).

**OUTPUT FORMAT (B·∫ÆT BU·ªòC JSON):**
Ch·ªâ tr·∫£ v·ªÅ 1 JSON duy nh·∫•t.
L∆ØU √ù QUAN TR·ªåNG:
1. Kh√¥ng d√πng Markdown (```json ... ```). Tr·∫£ v·ªÅ raw text.
2. KH√îNG ƒê∆Ø·ª¢C c√≥ d·∫•u ph·∫©y (,) ·ªü cu·ªëi danh s√°ch ho·∫∑c object cu·ªëi c√πng. (NO TRAILING COMMAS).
3. ƒê·∫£m b·∫£o c·∫•u tr√∫c ngo·∫∑c {{}} ƒë√≥ng m·ªü ch√≠nh x√°c. 

C·∫•u tr√∫c nh∆∞ sau:
{{
    "personal_info": {{
        "name": "String",
        "position": "String (Single title only, e.g., 'Backend Developer')",
        "experience": "String (Single value only, e.g., '2 years')"
    }},
    "matching_score": {{
        "percentage": Integer,
        "explanation": "String (e.g., 'Matched 8/10 requirements')"
    }},
    "requirements_breakdown": {{
        "must_have_ratio": "String (e.g., '5/7')",
        "nice_to_have_ratio": "String (e.g., '3/3')"
    }},
    "matched_keywords": ["String", "String", ...],
    "radar_chart": {{
        "Hard Skills": Integer,
        "Soft Skills": Integer,
        "Experience": Integer,
        "Education": Integer,
        "Domain Knowledge": Integer
    }},
    "radar_reasoning": {{
        "Hard Skills": {{ "en": "English explanation...", "vi": "Gi·∫£i th√≠ch ti·∫øng Vi·ªát..." }},
        "Soft Skills": {{ "en": "...", "vi": "..." }},
        "Experience": {{ "en": "...", "vi": "..." }},
        "Education": {{ "en": "...", "vi": "..." }},
        "Domain Knowledge": {{ "en": "...", "vi": "..." }}
    }},
    "bilingual_content": {{
        "general_assessment": {{
            "en": "String",
            "vi": "String"
        }},
        "comparison_table": [
            {{
                "jd_requirement": "String (Original JD line)",
                "cv_evidence": "String (Evidence from CV or 'Not found')",
                "status": "Matched/Not Matched"
            }}
        ],
        "strengths": {{
            "en": ["String", "String"],
            "vi": ["String", "String"]
        }},
        "weaknesses_missing_skills": {{
            "en": ["String", "String"],
            "vi": ["String", "String"]
        }},
        "interview_questions": {{
            "en": ["String", "String"],
            "vi": ["String", "String"]
        }}
    }}
}}
"""

_llm_instance = None
_embedding_instance = None

def get_llm():
    global _llm_instance
    if _llm_instance is None:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key: print("‚ùå L·ªñI: GOOGLE_API_KEY ch∆∞a c·∫•u h√¨nh!")
        
        # C·∫•u h√¨nh Safety Settings ƒë·ªÉ kh√¥ng b·ªã ch·∫∑n khi ƒë·ªçc CV c√° nh√¢n
        _llm_instance = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash", 
            temperature=0.2,
            google_api_key=api_key,
            safety_settings={
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            }
        )
    return _llm_instance

def get_embeddings():
    global _embedding_instance
    if _embedding_instance is None:
        _embedding_instance = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    return _embedding_instance

# --- [QUAN TR·ªåNG] H√ÄM V·ªÜ SINH TEXT ƒê·ªÇ TR√ÅNH L·ªñI LANGCHAIN ---
def sanitize_text_for_prompt(text):
    if not text: return ""
    # Thay th·∫ø d·∫•u ngo·∫∑c nh·ªçn b·∫±ng ngo·∫∑c tr√≤n ƒë·ªÉ LangChain kh√¥ng hi·ªÉu nh·∫ßm l√† bi·∫øn
    text = text.replace("{", "(").replace("}", ")")
    return text

def clean_json_string(json_str):
    try:
        start_idx = json_str.find('{')
        end_idx = json_str.rfind('}')
        if start_idx != -1 and end_idx != -1:
            json_str = json_str[start_idx : end_idx + 1]
        json_str = re.sub(r",\s*([\]}])", r"\1", json_str)
        return json_str
    except Exception:
        return json_str 

def analyze_cv_logic(file_path: str, jd_text: str):
    if not os.getenv("GOOGLE_API_KEY"):
        return {"error": "Server Config Error: Missing GOOGLE_API_KEY"}

    # 1. X·ª≠ l√Ω PDF (D√πng pdfplumber tr·ª±c ti·∫øp ƒë·ªÉ robust h∆°n v·ªõi file ·∫£nh/layout l·∫°)
    full_text = ""
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                # extract_text() x·ª≠ l√Ω t·ªët layout c·ªôt
                page_text = page.extract_text() or "" 
                full_text += page_text + "\n"
        
        if not full_text.strip():
            return {"error": "Kh√¥ng th·ªÉ ƒë·ªçc ch·ªØ t·ª´ file PDF n√†y (c√≥ th·ªÉ l√† file ·∫£nh scan)."}
            
        # [FIX] V·ªá sinh vƒÉn b·∫£n ngay sau khi ƒë·ªçc
        full_text = sanitize_text_for_prompt(full_text)
        jd_text = sanitize_text_for_prompt(jd_text)

        # Chia nh·ªè vƒÉn b·∫£n
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        # T·∫°o docs gi·∫£ l·∫≠p t·ª´ text ƒë√£ v·ªá sinh
        docs = text_splitter.create_documents([full_text])
        
    except Exception as e:
        return {"error": f"L·ªói ƒë·ªçc file PDF: {str(e)}"}

    # 2. RAG & Gemini
    try:
        embeddings = get_embeddings()
        llm = get_llm()

        vectorstore = Chroma.from_documents(
            documents=docs,
            embedding=embeddings,
            collection_name=f"cv_analysis_{int(time.time())}",
        )
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        
        prompt = ChatPromptTemplate.from_template(CORE_PROMPT)

        def format_docs(docs_list):
            return "\n\n".join(d.page_content for d in docs_list)

        # Retrieve context th·ªß c√¥ng ƒë·ªÉ ki·ªÉm so√°t d·ªØ li·ªáu v√†o
        relevant_docs = retriever.get_relevant_documents(jd_text)
        context_text = format_docs(relevant_docs)

        # G·ªçi LLM tr·ª±c ti·∫øp (kh√¥ng d√πng chain ph·ª©c t·∫°p ƒë·ªÉ tr√°nh l·ªói bi·∫øn)
        final_prompt_value = prompt.format_messages(
            cv_text=context_text, 
            jd_text=jd_text
        )
        
        print("ü§ñ Analyzing with Gemini 1.5 Flash...")
        response = llm.invoke(final_prompt_value)
        
        # X·ª≠ l√Ω k·∫øt qu·∫£
        raw_content = response.content
        print(f"üîç Raw Output: {raw_content[:50]}...") 
        
        cleaned_content = clean_json_string(raw_content)
        
        try:
            result = json.loads(cleaned_content)
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON Error: {str(e)}")
            return {"error": "AI tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON kh√¥ng h·ª£p l·ªá. Vui l√≤ng th·ª≠ l·∫°i."}
        
        vectorstore.delete_collection() 
        return result

    except Exception as e:
        print(f"‚ùå System Error: {str(e)}")
        return {"error": f"System Error: {str(e)}"}
