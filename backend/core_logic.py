# backend/core_logic.py
import os
import time
import shutil
from dotenv import load_dotenv
from typing import List, Dict, Union

# --- LangChain Modern Imports ---
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma  # New package
from langchain_huggingface import HuggingFaceEmbeddings # New package
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnablePassthrough
from pydantic import BaseModel, Field

# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# --- DATA MODELS (Pydantic v2) ---
class JobMatchResult(BaseModel):
    personal_info: Dict[str, str] = Field(description="Name, position, experience extracted from CV")
    matching_score: Dict[str, Union[int, str]] = Field(description="Percentage score and explanation")
    requirements_breakdown: Dict[str, str] = Field(description="Ratios for must-have and nice-to-have criteria")
    matched_keywords: List[str] = Field(description="List of matching technical skills")
    radar_chart: Dict[str, int] = Field(description="Scores 1-10 for 5 dimensions")
    bilingual_content: Dict[str, Union[Dict, List]] = Field(description="Assessment content in EN and VI")

# --- PROMPT (Gi·ªØ nguy√™n logic c≈©) ---
CORE_PROMPT = """
B·∫°n l√† m·ªôt Tr·ª£ l√Ω Tuy·ªÉn d·ª•ng AI chuy√™n nghi·ªáp (JobMatchr). Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch CV (ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi d·∫°ng text) v√† M√¥ t·∫£ c√¥ng vi·ªác (JD - m·ªói d√≤ng l√† m·ªôt y√™u c·∫ßu).

**INPUT DATA:**
1. CV Text: {cv_text}
2. JD Text: {jd_text} (L∆∞u √Ω: M·ªói d√≤ng trong JD l√† m·ªôt ti√™u ch√≠ ri√™ng bi·ªát).

**NHI·ªÜM V·ª§:**
H√£y th·ª±c hi·ªán c√°c b∆∞·ªõc sau m·ªôt c√°ch logic:

B∆Ø·ªöC 1: TR√çCH XU·∫§T TH√îNG TIN C√Å NH√ÇN
- T√¨m Name, Position (V·ªã tr√≠ ·ª©ng tuy·ªÉn/hi·ªán t·∫°i), Experience (T·ªïng s·ªë nƒÉm kinh nghi·ªám - ch·ªâ l·∫•y s·ªë).

B∆Ø·ªöC 2: PH√ÇN T√çCH JD V√Ä T√çNH ƒêI·ªÇM (QUY T·∫ÆC "1 ƒê·ªÄU")
- T√°ch JD th√†nh c√°c d√≤ng ri√™ng bi·ªát. T·ªïng s·ªë d√≤ng = T·ªïng y√™u c·∫ßu (Total_Req).
- Ph√¢n lo·∫°i t·ª´ng d√≤ng th√†nh "B·∫Øt bu·ªôc" (Requirement) ho·∫∑c "∆Øu ti√™n" (Nice-to-have) d·ª±a tr√™n t·ª´ kh√≥a (n·∫øu kh√¥ng r√µ, m·∫∑c ƒë·ªãnh l√† B·∫Øt bu·ªôc).
- ƒê·ªëi chi·∫øu CV: V·ªõi m·ªói d√≤ng JD, n·∫øu CV c√≥ b·∫±ng ch·ª©ng ƒë√°p ·ª©ng => T√≠nh l√† 1 ƒëi·ªÉm (Matched).
- Keyword ph√°t hi·ªán: Tr√≠ch xu·∫•t c√°c t·ª´ kh√≥a k·ªπ thu·∫≠t (Hard skill) tr√πng kh·ªõp gi·ªØa CV v√† JD.
- C√¥ng th·ª©c t√≠nh % chung: (T·ªïng s·ªë d√≤ng Matched / T·ªïng s·ªë d√≤ng JD) * 100.

B∆Ø·ªöC 3: ƒê√ÅNH GI√Å SONG NG·ªÆ (ANH & VI·ªÜT)
- T·∫°o n·ªôi dung ƒë√°nh gi√° cho c√°c m·ª•c: ƒê√°nh gi√° chung, ƒêi·ªÉm m·∫°nh, ƒêi·ªÉm y·∫øu (Missing skills), C√¢u h·ªèi ph·ªèng v·∫•n.
- N·ªôi dung Ti·∫øng Anh vi·∫øt tr∆∞·ªõc, Ti·∫øng Vi·ªát d·ªãch s√°t nghƒ©a theo sau.

B∆Ø·ªöC 4: CH·∫§M ƒêI·ªÇM RADAR CHART (Thang 1-10)
- ƒê√°nh gi√° 5 kh√≠a c·∫°nh: Hard Skills, Soft Skills, Experience, Education, Domain Knowledge.

**OUTPUT FORMAT (B·∫ÆT BU·ªòC JSON):**
Ch·ªâ tr·∫£ v·ªÅ 1 JSON duy nh·∫•t, kh√¥ng c√≥ markdown, kh√¥ng c√≥ l·ªùi d·∫´n. C·∫•u tr√∫c nh∆∞ sau:
{{
    "personal_info": {{
        "name": "String",
        "position": "String (Single title only, e.g., 'Backend Developer')",
        "experience": "String (Single value only, e.g., '2 years')"
    }},
    "matching_score": {{
        "percentage": Integer,
        "explanation": "String (e.g., 'Matched 8/10 requirements')"
    }},
    "requirements_breakdown": {{
        "must_have_ratio": "String (e.g., '5/7')",
        "nice_to_have_ratio": "String (e.g., '3/3')"
    }},
    "matched_keywords": ["String", "String", ...],
    "radar_chart": {{
        "Hard Skills": Integer,
        "Soft Skills": Integer,
        "Experience": Integer,
        "Education": Integer,
        "Domain Knowledge": Integer
    }},
    "bilingual_content": {{
        "general_assessment": {{
            "en": "String",
            "vi": "String"
        }},
        "comparison_table": [
            {{
                "jd_requirement": "String (Original JD line)",
                "cv_evidence": "String (Evidence from CV or 'Not found')",
                "status": "Matched/Not Matched"
            }}
        ],
        "strengths": {{
            "en": ["String", "String"],
            "vi": ["String", "String"]
        }},
        "weaknesses_missing_skills": {{
            "en": ["String", "String"],
            "vi": ["String", "String"]
        }},
        "interview_questions": {{
            "en": ["String", "String"],
            "vi": ["String", "String"]
        }}
    }}
}}
"""

# --- KH·ªûI T·∫†O RESOURCES (Lazy loading t·ªët h∆°n cho Server) ---
def get_llm():
    # S·ª≠ d·ª•ng gemini-1.5-flash v√¨ nhanh v√† r·∫ª h∆°n cho t√°c v·ª• ƒë·ªçc vƒÉn b·∫£n
    return ChatGoogleGenerativeAI(
        model="gemini-flash-latest", 
        temperature=0.2,
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )

def get_embeddings():
    # S·ª≠ d·ª•ng device='cpu' ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫°y ƒë∆∞·ª£c tr√™n m·ªçi server th∆∞·ªùng
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

def analyze_cv_logic(file_path: str, jd_text: str):
    """
    X·ª≠ l√Ω logic ch√≠nh: ƒê·ªçc PDF -> Vector Store -> LLM -> JSON
    """
    llm = get_llm()
    embeddings = get_embeddings()

    if not os.getenv("GOOGLE_API_KEY"):
        return {"error": "GOOGLE_API_KEY not found in .env"}

    # 1. X·ª≠ l√Ω PDF
    try:
        loader = PDFPlumberLoader(file_path)
        docs = loader.load()
        if not docs:
            return {"error": "Kh√¥ng th·ªÉ ƒë·ªçc n·ªôi dung t·ª´ file PDF."}
            
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
    except Exception as e:
        return {"error": f"L·ªói ƒë·ªçc PDF: {str(e)}"}

    # 2. Vector Store (In-Memory cho m·ªói Request ƒë·ªÉ tr√°nh r√°c ·ªï c·ª©ng)
    # V·ªõi Python 3.13 v√† LangChain m·ªõi, ta kh√¥ng c·∫ßn persist xu·ªëng ƒëƒ©a cho t√°c v·ª• n√†y
    try:
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            collection_name=f"cv_analysis_{int(time.time())}",
            # Kh√¥ng set persist_directory ƒë·ªÉ ch·∫°y in-memory (nhanh h∆°n v√† t·ª± h·ªßy khi xong)
        )
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    except Exception as e:
        return {"error": f"L·ªói kh·ªüi t·∫°o Vector DB: {str(e)}"}

    # 3. ƒê·ªãnh nghƒ©a Chain
    parser = JsonOutputParser(pydantic_object=JobMatchResult)
    
    prompt = ChatPromptTemplate.from_template(CORE_PROMPT)
    
    # Inject format instructions v√†o prompt
    prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    def format_docs(docs):
        return "\n\n".join(d.page_content for d in docs)

    chain = (
        {"cv_text": retriever | format_docs, "jd_text": RunnablePassthrough()}
        | prompt
        | llm
        | parser
    )

    # 4. Ch·∫°y v√† tr·∫£ v·ªÅ k·∫øt qu·∫£
    try:
        print("ü§ñ ƒêang ph√¢n t√≠ch v·ªõi Gemini 1.5 Flash...")
        result = chain.invoke(jd_text)
        
        # Cleanup th·ªß c√¥ng n·∫øu c·∫ßn (d√π in-memory s·∫Ω t·ª± gi·∫£i ph√≥ng)
        vectorstore.delete_collection() 
        
        return result
    except Exception as e:
        return {"error": f"L·ªói ph√¢n t√≠ch AI: {str(e)}"}